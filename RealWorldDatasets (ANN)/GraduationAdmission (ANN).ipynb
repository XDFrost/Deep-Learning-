{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is a regression problem hence activation function of output layer is linear."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Admission_Predict_Ver1.1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 9)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Serial No.</th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Serial No.  GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  \\\n",
       "0           1        337          118                  4  4.5   4.5  9.65   \n",
       "1           2        324          107                  4  4.0   4.5  8.87   \n",
       "2           3        316          104                  3  3.0   3.5  8.00   \n",
       "3           4        322          110                  3  3.5   2.5  8.67   \n",
       "4           5        314          103                  2  2.0   3.0  8.21   \n",
       "\n",
       "   Research  Chance of Admit   \n",
       "0         1              0.92  \n",
       "1         1              0.76  \n",
       "2         1              0.72  \n",
       "3         1              0.80  \n",
       "4         0              0.65  "
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(df.shape)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- GRE Scores ( out of 340 )\n",
    "\n",
    "- TOEFL Scores ( out of 120 )\n",
    "\n",
    "- University Rating ( out of 5 )\n",
    "\n",
    "- Statement of Purpose and Letter of Recommendation Strength ( out of 5 )\n",
    "\n",
    "- Undergraduate GPA ( out of 10 )\n",
    "\n",
    "- Research Experience ( either 0 or 1 )\n",
    "\n",
    "- Chance of Admit ( ranging from 0 to 1 )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 500 entries, 0 to 499\n",
      "Data columns (total 9 columns):\n",
      " #   Column             Non-Null Count  Dtype  \n",
      "---  ------             --------------  -----  \n",
      " 0   Serial No.         500 non-null    int64  \n",
      " 1   GRE Score          500 non-null    int64  \n",
      " 2   TOEFL Score        500 non-null    int64  \n",
      " 3   University Rating  500 non-null    int64  \n",
      " 4   SOP                500 non-null    float64\n",
      " 5   LOR                500 non-null    float64\n",
      " 6   CGPA               500 non-null    float64\n",
      " 7   Research           500 non-null    int64  \n",
      " 8   Chance of Admit    500 non-null    float64\n",
      "dtypes: float64(4), int64(5)\n",
      "memory usage: 35.3 KB\n"
     ]
    }
   ],
   "source": [
    "# Checking for missing values\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Duplicated rows\n",
    "\n",
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.drop(columns= df.columns[:1], axis = 1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "      <th>Chance of Admit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "      <td>0.92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "      <td>0.76</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "      <td>0.72</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "      <td>0.80</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "      <td>0.65</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research  \\\n",
       "0        337          118                  4  4.5   4.5  9.65         1   \n",
       "1        324          107                  4  4.0   4.5  8.87         1   \n",
       "2        316          104                  3  3.0   3.5  8.00         1   \n",
       "3        322          110                  3  3.5   2.5  8.67         1   \n",
       "4        314          103                  2  2.0   3.0  8.21         0   \n",
       "\n",
       "   Chance of Admit   \n",
       "0              0.92  \n",
       "1              0.76  \n",
       "2              0.72  \n",
       "3              0.80  \n",
       "4              0.65  "
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.iloc[:,:-1]\n",
    "y = df.iloc[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>337</td>\n",
       "      <td>118</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.65</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>324</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>8.87</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>316</td>\n",
       "      <td>104</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.00</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>322</td>\n",
       "      <td>110</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.67</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>314</td>\n",
       "      <td>103</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.21</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>495</th>\n",
       "      <td>332</td>\n",
       "      <td>108</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.02</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>496</th>\n",
       "      <td>337</td>\n",
       "      <td>117</td>\n",
       "      <td>5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.87</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>497</th>\n",
       "      <td>330</td>\n",
       "      <td>120</td>\n",
       "      <td>5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>9.56</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>498</th>\n",
       "      <td>312</td>\n",
       "      <td>103</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.43</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>499</th>\n",
       "      <td>327</td>\n",
       "      <td>113</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.04</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>500 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research\n",
       "0          337          118                  4  4.5   4.5  9.65         1\n",
       "1          324          107                  4  4.0   4.5  8.87         1\n",
       "2          316          104                  3  3.0   3.5  8.00         1\n",
       "3          322          110                  3  3.5   2.5  8.67         1\n",
       "4          314          103                  2  2.0   3.0  8.21         0\n",
       "..         ...          ...                ...  ...   ...   ...       ...\n",
       "495        332          108                  5  4.5   4.0  9.02         1\n",
       "496        337          117                  5  5.0   5.0  9.87         1\n",
       "497        330          120                  5  4.5   5.0  9.56         1\n",
       "498        312          103                  4  4.0   5.0  8.43         0\n",
       "499        327          113                  4  4.5   4.5  9.04         0\n",
       "\n",
       "[500 rows x 7 columns]"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0      0.92\n",
       "1      0.76\n",
       "2      0.72\n",
       "3      0.80\n",
       "4      0.65\n",
       "       ... \n",
       "495    0.87\n",
       "496    0.96\n",
       "497    0.93\n",
       "498    0.73\n",
       "499    0.84\n",
       "Name: Chance of Admit , Length: 500, dtype: float64"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(X, y, test_size=.2, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We'll use min-max scaling as upper bound and lower bound are given for each feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>249</th>\n",
       "      <td>321</td>\n",
       "      <td>111</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.83</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>433</th>\n",
       "      <td>316</td>\n",
       "      <td>111</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>8.54</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>303</td>\n",
       "      <td>102</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.50</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>322</th>\n",
       "      <td>314</td>\n",
       "      <td>107</td>\n",
       "      <td>2</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.27</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>308</td>\n",
       "      <td>106</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.21</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>106</th>\n",
       "      <td>329</td>\n",
       "      <td>111</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.5</td>\n",
       "      <td>9.18</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>270</th>\n",
       "      <td>306</td>\n",
       "      <td>105</td>\n",
       "      <td>2</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.22</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>348</th>\n",
       "      <td>302</td>\n",
       "      <td>99</td>\n",
       "      <td>1</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>7.25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>435</th>\n",
       "      <td>309</td>\n",
       "      <td>105</td>\n",
       "      <td>2</td>\n",
       "      <td>2.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>7.68</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>102</th>\n",
       "      <td>314</td>\n",
       "      <td>106</td>\n",
       "      <td>2</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.25</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>400 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research\n",
       "249        321          111                  3  3.5   4.0  8.83         1\n",
       "433        316          111                  4  4.0   5.0  8.54         0\n",
       "19         303          102                  3  3.5   3.0  8.50         0\n",
       "322        314          107                  2  2.5   4.0  8.27         0\n",
       "332        308          106                  3  3.5   2.5  8.21         1\n",
       "..         ...          ...                ...  ...   ...   ...       ...\n",
       "106        329          111                  4  4.5   4.5  9.18         1\n",
       "270        306          105                  2  2.5   3.0  8.22         1\n",
       "348        302           99                  1  2.0   2.0  7.25         0\n",
       "435        309          105                  2  2.5   4.0  7.68         0\n",
       "102        314          106                  2  4.0   3.5  8.25         0\n",
       "\n",
       "[400 rows x 7 columns]"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>GRE Score</th>\n",
       "      <th>TOEFL Score</th>\n",
       "      <th>University Rating</th>\n",
       "      <th>SOP</th>\n",
       "      <th>LOR</th>\n",
       "      <th>CGPA</th>\n",
       "      <th>Research</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>361</th>\n",
       "      <td>334</td>\n",
       "      <td>116</td>\n",
       "      <td>4</td>\n",
       "      <td>4.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>9.54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>314</td>\n",
       "      <td>108</td>\n",
       "      <td>4</td>\n",
       "      <td>4.5</td>\n",
       "      <td>4.0</td>\n",
       "      <td>9.04</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>315</td>\n",
       "      <td>105</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.5</td>\n",
       "      <td>7.65</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>312</td>\n",
       "      <td>109</td>\n",
       "      <td>3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>8.69</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>104</th>\n",
       "      <td>326</td>\n",
       "      <td>112</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>9.05</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>347</th>\n",
       "      <td>299</td>\n",
       "      <td>94</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>7.34</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>315</td>\n",
       "      <td>106</td>\n",
       "      <td>3</td>\n",
       "      <td>4.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.42</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>329</td>\n",
       "      <td>114</td>\n",
       "      <td>2</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>8.56</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>438</th>\n",
       "      <td>318</td>\n",
       "      <td>110</td>\n",
       "      <td>1</td>\n",
       "      <td>2.5</td>\n",
       "      <td>3.5</td>\n",
       "      <td>8.54</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>314</td>\n",
       "      <td>105</td>\n",
       "      <td>3</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.5</td>\n",
       "      <td>8.30</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>100 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     GRE Score  TOEFL Score  University Rating  SOP  LOR   CGPA  Research\n",
       "361        334          116                  4  4.0   3.5  9.54         1\n",
       "73         314          108                  4  4.5   4.0  9.04         1\n",
       "374        315          105                  2  2.0   2.5  7.65         0\n",
       "155        312          109                  3  3.0   3.0  8.69         0\n",
       "104        326          112                  3  3.5   3.0  9.05         1\n",
       "..         ...          ...                ...  ...   ...   ...       ...\n",
       "347        299           94                  1  1.0   1.0  7.34         0\n",
       "86         315          106                  3  4.5   3.5  8.42         0\n",
       "75         329          114                  2  2.0   4.0  8.56         1\n",
       "438        318          110                  1  2.5   3.5  8.54         1\n",
       "15         314          105                  3  3.5   2.5  8.30         0\n",
       "\n",
       "[100 rows x 7 columns]"
      ]
     },
     "execution_count": 197,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "249    0.77\n",
       "433    0.71\n",
       "19     0.62\n",
       "322    0.72\n",
       "332    0.75\n",
       "       ... \n",
       "106    0.87\n",
       "270    0.72\n",
       "348    0.57\n",
       "435    0.55\n",
       "102    0.62\n",
       "Name: Chance of Admit , Length: 400, dtype: float64"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yTrain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "361    0.93\n",
       "73     0.84\n",
       "374    0.39\n",
       "155    0.77\n",
       "104    0.74\n",
       "       ... \n",
       "347    0.42\n",
       "86     0.72\n",
       "75     0.72\n",
       "438    0.67\n",
       "15     0.54\n",
       "Name: Chance of Admit , Length: 100, dtype: float64"
      ]
     },
     "execution_count": 199,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "yTest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "scale = MinMaxScaler()\n",
    "\n",
    "xTrainScaled = scale.fit_transform(xTrain)\n",
    "xTestScaled = scale.fit_transform(xTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.62      , 0.67857143, 0.5       , ..., 0.71428571, 0.65064103,\n",
       "        1.        ],\n",
       "       [0.52      , 0.67857143, 0.75      , ..., 1.        , 0.55769231,\n",
       "        0.        ],\n",
       "       [0.26      , 0.35714286, 0.5       , ..., 0.42857143, 0.54487179,\n",
       "        0.        ],\n",
       "       ...,\n",
       "       [0.24      , 0.25      , 0.        , ..., 0.14285714, 0.14423077,\n",
       "        0.        ],\n",
       "       [0.38      , 0.46428571, 0.25      , ..., 0.71428571, 0.28205128,\n",
       "        0.        ],\n",
       "       [0.48      , 0.5       , 0.25      , ..., 0.57142857, 0.46474359,\n",
       "        0.        ]])"
      ]
     },
     "execution_count": 201,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTrainScaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.88      , 0.85185185, 0.75      , 0.75      , 0.625     ,\n",
       "        0.89430894, 1.        ],\n",
       "       [0.48      , 0.55555556, 0.75      , 0.875     , 0.75      ,\n",
       "        0.69105691, 1.        ],\n",
       "       [0.5       , 0.44444444, 0.25      , 0.25      , 0.375     ,\n",
       "        0.12601626, 0.        ],\n",
       "       [0.44      , 0.59259259, 0.5       , 0.5       , 0.5       ,\n",
       "        0.54878049, 0.        ],\n",
       "       [0.72      , 0.7037037 , 0.5       , 0.625     , 0.5       ,\n",
       "        0.69512195, 1.        ],\n",
       "       [0.78      , 0.66666667, 0.75      , 0.875     , 0.75      ,\n",
       "        0.76829268, 1.        ],\n",
       "       [0.        , 0.25925926, 0.        , 0.125     , 0.25      ,\n",
       "        0.08943089, 0.        ],\n",
       "       [0.22      , 0.48148148, 0.75      , 0.375     , 0.5       ,\n",
       "        0.45934959, 0.        ],\n",
       "       [0.56      , 0.59259259, 0.5       , 0.625     , 0.75      ,\n",
       "        0.76422764, 1.        ],\n",
       "       [0.6       , 0.7037037 , 0.75      , 0.5       , 0.875     ,\n",
       "        0.61788618, 1.        ],\n",
       "       [0.66      , 0.55555556, 0.5       , 0.625     , 0.5       ,\n",
       "        0.51219512, 0.        ],\n",
       "       [0.52      , 0.59259259, 0.5       , 0.625     , 0.5       ,\n",
       "        0.57723577, 0.        ],\n",
       "       [0.64      , 0.37037037, 0.75      , 0.5       , 0.375     ,\n",
       "        0.27642276, 1.        ],\n",
       "       [1.        , 0.81481481, 1.        , 0.875     , 0.875     ,\n",
       "        0.85772358, 1.        ],\n",
       "       [0.68      , 0.62962963, 0.5       , 0.625     , 0.5       ,\n",
       "        0.76422764, 1.        ],\n",
       "       [0.12      , 0.14814815, 0.25      , 0.125     , 0.25      ,\n",
       "        0.18699187, 0.        ],\n",
       "       [0.84      , 0.55555556, 1.        , 0.875     , 0.75      ,\n",
       "        0.68292683, 1.        ],\n",
       "       [0.2       , 0.14814815, 0.25      , 0.5       , 0.5       ,\n",
       "        0.30894309, 1.        ],\n",
       "       [0.16      , 0.2962963 , 0.25      , 0.125     , 0.25      ,\n",
       "        0.21138211, 0.        ],\n",
       "       [0.14      , 0.2962963 , 0.5       , 0.25      , 0.75      ,\n",
       "        0.13414634, 1.        ],\n",
       "       [0.34      , 0.44444444, 0.25      , 0.375     , 0.875     ,\n",
       "        0.31707317, 1.        ],\n",
       "       [0.14      , 0.22222222, 0.75      , 0.5       , 0.625     ,\n",
       "        0.19105691, 0.        ],\n",
       "       [0.42      , 0.33333333, 0.5       , 0.875     , 0.75      ,\n",
       "        0.52845528, 1.        ],\n",
       "       [0.74      , 0.59259259, 0.5       , 0.625     , 0.75      ,\n",
       "        0.58130081, 1.        ],\n",
       "       [0.74      , 0.7037037 , 0.5       , 0.5       , 0.5       ,\n",
       "        0.56097561, 1.        ],\n",
       "       [0.22      , 0.33333333, 0.5       , 0.375     , 0.25      ,\n",
       "        0.32113821, 1.        ],\n",
       "       [0.8       , 1.        , 1.        , 0.875     , 1.        ,\n",
       "        0.90243902, 1.        ],\n",
       "       [0.76      , 0.62962963, 0.75      , 1.        , 0.75      ,\n",
       "        0.73170732, 1.        ],\n",
       "       [0.44      , 0.44444444, 0.25      , 0.375     , 0.5       ,\n",
       "        0.31707317, 0.        ],\n",
       "       [0.44      , 0.51851852, 0.75      , 0.875     , 0.75      ,\n",
       "        0.53252033, 1.        ],\n",
       "       [0.2       , 0.2962963 , 0.5       , 0.625     , 0.375     ,\n",
       "        0.2195122 , 0.        ],\n",
       "       [0.58      , 0.62962963, 0.5       , 0.5       , 0.375     ,\n",
       "        0.58943089, 0.        ],\n",
       "       [0.3       , 0.40740741, 0.25      , 0.375     , 0.125     ,\n",
       "        0.18292683, 0.        ],\n",
       "       [0.66      , 0.74074074, 0.75      , 0.75      , 0.875     ,\n",
       "        0.76829268, 1.        ],\n",
       "       [0.52      , 0.40740741, 0.5       , 0.5       , 0.625     ,\n",
       "        0.26829268, 1.        ],\n",
       "       [0.58      , 0.55555556, 0.5       , 0.5       , 0.625     ,\n",
       "        0.48780488, 1.        ],\n",
       "       [0.2       , 0.18518519, 0.        , 0.25      , 0.375     ,\n",
       "        0.27642276, 0.        ],\n",
       "       [0.84      , 0.92592593, 1.        , 1.        , 1.        ,\n",
       "        0.93495935, 1.        ],\n",
       "       [0.22      , 0.40740741, 0.5       , 0.625     , 0.75      ,\n",
       "        0.31707317, 1.        ],\n",
       "       [0.5       , 0.51851852, 0.25      , 0.75      , 0.5       ,\n",
       "        0.47154472, 1.        ],\n",
       "       [1.        , 0.74074074, 0.75      , 1.        , 1.        ,\n",
       "        0.97560976, 1.        ],\n",
       "       [0.22      , 0.14814815, 0.25      , 0.5       , 0.5       ,\n",
       "        0.2195122 , 1.        ],\n",
       "       [0.42      , 0.2962963 , 0.25      , 0.375     , 0.625     ,\n",
       "        0.40650407, 1.        ],\n",
       "       [0.74      , 0.74074074, 0.75      , 0.875     , 0.875     ,\n",
       "        0.7195122 , 1.        ],\n",
       "       [1.        , 0.77777778, 1.        , 0.75      , 0.75      ,\n",
       "        0.91869919, 1.        ],\n",
       "       [0.22      , 0.22222222, 0.25      , 0.5       , 0.25      ,\n",
       "        0.35772358, 0.        ],\n",
       "       [0.94      , 0.92592593, 0.75      , 0.875     , 0.875     ,\n",
       "        0.93902439, 1.        ],\n",
       "       [0.74      , 0.66666667, 0.75      , 0.75      , 0.875     ,\n",
       "        0.67479675, 1.        ],\n",
       "       [0.74      , 0.48148148, 0.75      , 0.75      , 0.875     ,\n",
       "        0.57317073, 1.        ],\n",
       "       [0.76      , 0.85185185, 1.        , 1.        , 1.        ,\n",
       "        0.87804878, 1.        ],\n",
       "       [0.62      , 0.66666667, 1.        , 1.        , 1.        ,\n",
       "        0.85772358, 1.        ],\n",
       "       [0.18      , 0.25925926, 0.25      , 0.5       , 0.625     ,\n",
       "        0.2195122 , 0.        ],\n",
       "       [0.26      , 0.44444444, 1.        , 1.        , 0.875     ,\n",
       "        0.53252033, 0.        ],\n",
       "       [0.22      , 0.11111111, 0.        , 0.5       , 0.75      ,\n",
       "        0.08943089, 0.        ],\n",
       "       [0.92      , 0.92592593, 1.        , 0.875     , 1.        ,\n",
       "        0.8902439 , 1.        ],\n",
       "       [0.6       , 0.37037037, 0.5       , 0.5       , 0.5       ,\n",
       "        0.14634146, 0.        ],\n",
       "       [0.98      , 0.85185185, 0.75      , 0.75      , 0.625     ,\n",
       "        1.        , 1.        ],\n",
       "       [0.56      , 0.62962963, 0.5       , 0.75      , 0.5       ,\n",
       "        0.59349593, 0.        ],\n",
       "       [0.16      , 0.44444444, 0.5       , 0.625     , 0.75      ,\n",
       "        0.48780488, 0.        ],\n",
       "       [0.12      , 0.07407407, 0.25      , 0.5       , 0.25      ,\n",
       "        0.08130081, 1.        ],\n",
       "       [0.22      , 0.22222222, 0.5       , 0.375     , 0.25      ,\n",
       "        0.45121951, 1.        ],\n",
       "       [0.36      , 0.37037037, 0.25      , 0.375     , 0.75      ,\n",
       "        0.41463415, 1.        ],\n",
       "       [0.3       , 0.33333333, 0.25      , 0.25      , 0.375     ,\n",
       "        0.34146341, 0.        ],\n",
       "       [0.28      , 0.37037037, 1.        , 1.        , 0.5       ,\n",
       "        0.23577236, 0.        ],\n",
       "       [0.08      , 0.        , 0.        , 0.125     , 0.25      ,\n",
       "        0.00813008, 0.        ],\n",
       "       [0.34      , 0.55555556, 0.25      , 0.75      , 0.625     ,\n",
       "        0.14634146, 0.        ],\n",
       "       [0.68      , 0.74074074, 0.75      , 0.875     , 0.875     ,\n",
       "        0.77642276, 1.        ],\n",
       "       [0.78      , 0.77777778, 1.        , 0.75      , 1.        ,\n",
       "        0.79674797, 1.        ],\n",
       "       [0.4       , 0.40740741, 0.5       , 0.25      , 0.625     ,\n",
       "        0.41869919, 0.        ],\n",
       "       [0.56      , 0.48148148, 0.5       , 0.25      , 0.5       ,\n",
       "        0.53252033, 0.        ],\n",
       "       [0.26      , 0.25925926, 0.25      , 0.5       , 0.625     ,\n",
       "        0.29268293, 1.        ],\n",
       "       [0.72      , 0.62962963, 0.5       , 0.625     , 0.625     ,\n",
       "        0.57723577, 1.        ],\n",
       "       [0.6       , 0.2962963 , 0.25      , 0.375     , 0.5       ,\n",
       "        0.5203252 , 0.        ],\n",
       "       [0.34      , 0.44444444, 0.25      , 0.375     , 0.5       ,\n",
       "        0.12601626, 0.        ],\n",
       "       [0.2       , 0.22222222, 0.        , 0.        , 0.375     ,\n",
       "        0.27235772, 0.        ],\n",
       "       [0.46      , 0.48148148, 0.25      , 0.375     , 0.25      ,\n",
       "        0.44308943, 0.        ],\n",
       "       [0.74      , 0.74074074, 0.75      , 0.875     , 1.        ,\n",
       "        0.73170732, 0.        ],\n",
       "       [0.76      , 0.81481481, 0.75      , 0.875     , 0.75      ,\n",
       "        0.7398374 , 1.        ],\n",
       "       [0.3       , 0.33333333, 0.25      , 0.125     , 0.375     ,\n",
       "        0.12195122, 0.        ],\n",
       "       [0.3       , 0.11111111, 0.75      , 0.5       , 0.875     ,\n",
       "        0.37398374, 0.        ],\n",
       "       [0.72      , 0.55555556, 0.5       , 0.5       , 0.625     ,\n",
       "        0.6300813 , 0.        ],\n",
       "       [0.6       , 0.62962963, 1.        , 1.        , 0.875     ,\n",
       "        0.76422764, 1.        ],\n",
       "       [0.38      , 0.44444444, 0.75      , 0.625     , 0.25      ,\n",
       "        0.34146341, 0.        ],\n",
       "       [0.64      , 0.62962963, 0.75      , 0.75      , 1.        ,\n",
       "        0.72764228, 1.        ],\n",
       "       [0.66      , 0.40740741, 0.5       , 0.75      , 0.75      ,\n",
       "        0.44715447, 1.        ],\n",
       "       [0.56      , 0.48148148, 0.25      , 0.75      , 0.75      ,\n",
       "        0.23577236, 1.        ],\n",
       "       [0.2       , 0.40740741, 0.5       , 0.625     , 0.5       ,\n",
       "        0.33333333, 0.        ],\n",
       "       [0.62      , 0.62962963, 0.75      , 0.625     , 0.75      ,\n",
       "        0.41056911, 1.        ],\n",
       "       [0.42      , 0.51851852, 0.75      , 0.875     , 0.875     ,\n",
       "        0.67479675, 1.        ],\n",
       "       [0.36      , 0.37037037, 0.25      , 0.5       , 0.625     ,\n",
       "        0.46747967, 0.        ],\n",
       "       [0.58      , 0.44444444, 0.5       , 0.5       , 0.625     ,\n",
       "        0.54065041, 1.        ],\n",
       "       [0.74      , 0.85185185, 0.75      , 0.75      , 0.875     ,\n",
       "        0.8699187 , 1.        ],\n",
       "       [0.96      , 0.81481481, 1.        , 0.875     , 1.        ,\n",
       "        0.76829268, 1.        ],\n",
       "       [0.5       , 0.40740741, 0.5       , 0.5       , 0.375     ,\n",
       "        0.40243902, 0.        ],\n",
       "       [0.6       , 0.7037037 , 0.25      , 0.625     , 0.625     ,\n",
       "        0.58536585, 1.        ],\n",
       "       [0.18      , 0.03703704, 0.        , 0.        , 0.        ,\n",
       "        0.        , 0.        ],\n",
       "       [0.5       , 0.48148148, 0.5       , 0.875     , 0.625     ,\n",
       "        0.43902439, 0.        ],\n",
       "       [0.78      , 0.77777778, 0.25      , 0.25      , 0.75      ,\n",
       "        0.49593496, 1.        ],\n",
       "       [0.56      , 0.62962963, 0.        , 0.375     , 0.625     ,\n",
       "        0.48780488, 1.        ],\n",
       "       [0.48      , 0.44444444, 0.5       , 0.625     , 0.375     ,\n",
       "        0.3902439 , 0.        ]])"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTestScaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "from tensorflow import keras\n",
    "from keras import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programming lang\\PYTHON\\Deep Learning Notebooks\\.venv\\Lib\\site-packages\\keras\\src\\layers\\core\\dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(7, activation='relu', input_dim = 7))\n",
    "model.add(Dense(7, activation='relu'))\n",
    "model.add(Dense(1, activation='linear'))            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_6\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_6\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │            <span style=\"color: #00af00; text-decoration-color: #00af00\">56</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)              │             <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ dense_17 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │            \u001b[38;5;34m56\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_18 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │            \u001b[38;5;34m56\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ dense_19 (\u001b[38;5;33mDense\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1\u001b[0m)              │             \u001b[38;5;34m8\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">120</span> (480.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m120\u001b[0m (480.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">120</span> (480.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m120\u001b[0m (480.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(loss = 'mean_squared_error', optimizer='Adam')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 0.1055 - val_loss: 0.0929\n",
      "Epoch 2/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 0.0733 - val_loss: 0.0588\n",
      "Epoch 3/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0522 - val_loss: 0.0466\n",
      "Epoch 4/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0443 - val_loss: 0.0417\n",
      "Epoch 5/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0385 - val_loss: 0.0380\n",
      "Epoch 6/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0319 - val_loss: 0.0333\n",
      "Epoch 7/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0275 - val_loss: 0.0282\n",
      "Epoch 8/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0230 - val_loss: 0.0237\n",
      "Epoch 9/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0213 - val_loss: 0.0210\n",
      "Epoch 10/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0164 - val_loss: 0.0185\n",
      "Epoch 11/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0173 - val_loss: 0.0166\n",
      "Epoch 12/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0143 - val_loss: 0.0150\n",
      "Epoch 13/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0113 - val_loss: 0.0139\n",
      "Epoch 14/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0131 - val_loss: 0.0132\n",
      "Epoch 15/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0104 - val_loss: 0.0125\n",
      "Epoch 16/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0109 - val_loss: 0.0119\n",
      "Epoch 17/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0097 - val_loss: 0.0112\n",
      "Epoch 18/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0100 - val_loss: 0.0109\n",
      "Epoch 19/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0084 - val_loss: 0.0105\n",
      "Epoch 20/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0082 - val_loss: 0.0101\n",
      "Epoch 21/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0092 - val_loss: 0.0098\n",
      "Epoch 22/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0077 - val_loss: 0.0095\n",
      "Epoch 23/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0083 - val_loss: 0.0092\n",
      "Epoch 24/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0076 - val_loss: 0.0090\n",
      "Epoch 25/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0075 - val_loss: 0.0087\n",
      "Epoch 26/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0075 - val_loss: 0.0085\n",
      "Epoch 27/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0076 - val_loss: 0.0083\n",
      "Epoch 28/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0072 - val_loss: 0.0081\n",
      "Epoch 29/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0062 - val_loss: 0.0080\n",
      "Epoch 30/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0068 - val_loss: 0.0078\n",
      "Epoch 31/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0061 - val_loss: 0.0077\n",
      "Epoch 32/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0066 - val_loss: 0.0075\n",
      "Epoch 33/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0063 - val_loss: 0.0074\n",
      "Epoch 34/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0053 - val_loss: 0.0073\n",
      "Epoch 35/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0067 - val_loss: 0.0072\n",
      "Epoch 36/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0065 - val_loss: 0.0071\n",
      "Epoch 37/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0057 - val_loss: 0.0069\n",
      "Epoch 38/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0056 - val_loss: 0.0069\n",
      "Epoch 39/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.0058 - val_loss: 0.0068\n",
      "Epoch 40/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0053 - val_loss: 0.0067\n",
      "Epoch 41/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0050 - val_loss: 0.0066\n",
      "Epoch 42/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0055 - val_loss: 0.0066\n",
      "Epoch 43/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0053 - val_loss: 0.0064\n",
      "Epoch 44/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0050 - val_loss: 0.0064\n",
      "Epoch 45/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0057 - val_loss: 0.0063\n",
      "Epoch 46/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0050 - val_loss: 0.0063\n",
      "Epoch 47/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0055 - val_loss: 0.0061\n",
      "Epoch 48/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0048 - val_loss: 0.0061\n",
      "Epoch 49/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0051 - val_loss: 0.0061\n",
      "Epoch 50/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0052 - val_loss: 0.0060\n",
      "Epoch 51/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0048 - val_loss: 0.0059\n",
      "Epoch 52/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0052 - val_loss: 0.0059\n",
      "Epoch 53/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0052 - val_loss: 0.0058\n",
      "Epoch 54/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0054 - val_loss: 0.0057\n",
      "Epoch 55/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0048 - val_loss: 0.0057\n",
      "Epoch 56/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0043 - val_loss: 0.0056\n",
      "Epoch 57/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0046 - val_loss: 0.0056\n",
      "Epoch 58/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0048 - val_loss: 0.0055\n",
      "Epoch 59/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0050 - val_loss: 0.0055\n",
      "Epoch 60/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0042 - val_loss: 0.0054\n",
      "Epoch 61/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0049 - val_loss: 0.0054\n",
      "Epoch 62/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0050 - val_loss: 0.0053\n",
      "Epoch 63/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0052 - val_loss: 0.0053\n",
      "Epoch 64/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0046 - val_loss: 0.0053\n",
      "Epoch 65/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0041 - val_loss: 0.0053\n",
      "Epoch 66/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0049 - val_loss: 0.0052\n",
      "Epoch 67/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0046 - val_loss: 0.0052\n",
      "Epoch 68/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0044 - val_loss: 0.0051\n",
      "Epoch 69/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0041 - val_loss: 0.0051\n",
      "Epoch 70/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0039 - val_loss: 0.0051\n",
      "Epoch 71/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0045 - val_loss: 0.0051\n",
      "Epoch 72/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0042 - val_loss: 0.0050\n",
      "Epoch 73/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0040 - val_loss: 0.0050\n",
      "Epoch 74/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0045 - val_loss: 0.0049\n",
      "Epoch 75/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0046 - val_loss: 0.0049\n",
      "Epoch 76/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0038 - val_loss: 0.0049\n",
      "Epoch 77/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0046 - val_loss: 0.0049\n",
      "Epoch 78/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0044 - val_loss: 0.0049\n",
      "Epoch 79/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0043 - val_loss: 0.0048\n",
      "Epoch 80/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 0.0040 - val_loss: 0.0048\n",
      "Epoch 81/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0046 - val_loss: 0.0048\n",
      "Epoch 82/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0043 - val_loss: 0.0047\n",
      "Epoch 83/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0045 - val_loss: 0.0047\n",
      "Epoch 84/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0042 - val_loss: 0.0047\n",
      "Epoch 85/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 0.0043 - val_loss: 0.0047\n",
      "Epoch 86/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0040 - val_loss: 0.0047\n",
      "Epoch 87/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0041 - val_loss: 0.0047\n",
      "Epoch 88/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0039 - val_loss: 0.0046\n",
      "Epoch 89/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0041 - val_loss: 0.0046\n",
      "Epoch 90/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0049 - val_loss: 0.0046\n",
      "Epoch 91/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 0.0039 - val_loss: 0.0046\n",
      "Epoch 92/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0041 - val_loss: 0.0046\n",
      "Epoch 93/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0044 - val_loss: 0.0046\n",
      "Epoch 94/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0044 - val_loss: 0.0045\n",
      "Epoch 95/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0043 - val_loss: 0.0045\n",
      "Epoch 96/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0040 - val_loss: 0.0046\n",
      "Epoch 97/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0047 - val_loss: 0.0045\n",
      "Epoch 98/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0037 - val_loss: 0.0045\n",
      "Epoch 99/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0042 - val_loss: 0.0045\n",
      "Epoch 100/100\n",
      "\u001b[1m10/10\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 0.0039 - val_loss: 0.0045\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(xTrainScaled, yTrain, epochs=100, validation_split=.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4/4\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step \n"
     ]
    }
   ],
   "source": [
    "y_pred = model.predict(xTestScaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7965317676271553"
      ]
     },
     "execution_count": 209,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score            # r2 score is used to check accuracy of linear models\n",
    "\n",
    "r2_score(yTest, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x26484a70680>]"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
